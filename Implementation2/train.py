import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
import warnings
import pickle
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import f1_score, accuracy_score, classification_report
warnings.filterwarnings('ignore')

from config import RANDOM_SEED, FEATURE_COLUMNS, FAULT_TYPES
from data_prep import load_and_prepare_dataset
from models import train_edge_lightgbm_model, train_cloud_lstm_model

np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)

print("\n" + "="*80)
print(" " * 20 + "MODEL TRAINING & EVALUATION")
print(" " * 15 + "Automotive Predictive Maintenance System")
print("="*80)

print("\n[Step 1/6] Loading and preparing dataset...")
print("-" * 80)
df = load_and_prepare_dataset()
print(f"‚úì Dataset loaded: {df.shape}")
print(f"\nClass distribution:")
class_dist = df['Fault_Label'].value_counts().sort_index()
for idx, count in class_dist.items():
    pct = (count / len(df)) * 100
    print(f"  Class {idx} ({FAULT_TYPES[idx]:25s}): {count:5d} samples ({pct:5.2f}%)")

print("\n[Step 2/6] Splitting dataset...")
print("-" * 80)
X = df[FEATURE_COLUMNS]
y = df['Fault_Label']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y
)

print(f"‚úì Training set: {X_train.shape[0]} samples")
print(f"‚úì Test set: {X_test.shape[0]} samples")

print("\n[Step 3/6] Scaling features...")
print("-" * 80)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print(f"‚úì Features scaled (mean‚âà0, std‚âà1)")

print("\n[Step 4/6] Training Edge Model (LightGBM with SMOTE)...")
print("="* 80)
edge_model, edge_cm = train_edge_lightgbm_model(X_train_scaled, y_train, X_test_scaled, y_test)

print("\n[Step 5/6] Training Cloud Model (Fast Enhanced LSTM with SMOTE)...")
print("="* 80)
cloud_model, cloud_cm = train_cloud_lstm_model(X_train_scaled, y_train, X_test_scaled, y_test)

print("\n[Step 6/6] Saving models and scaler...")
print("-" * 80)

with open('edge_lightgbm_model.pkl', 'wb') as f:
    pickle.dump(edge_model, f)
print("‚úì Saved: edge_lightgbm_model.pkl")

cloud_model.save('cloud_lstm_model.keras')
print("‚úì Saved: cloud_lstm_model.keras")

with open('scaler.pkl', 'wb') as f:
    pickle.dump(scaler, f)
print("‚úì Saved: scaler.pkl")

np.save('edge_cm.npy', edge_cm)
np.save('cloud_cm.npy', cloud_cm)
print("‚úì Saved: Confusion matrices")

y_pred_edge = edge_model.predict(X_test_scaled)
y_pred_cloud = np.argmax(cloud_model.predict(X_test_scaled.reshape(-1, 1, X_test_scaled.shape[1]), verbose=0), axis=1)

edge_f1 = f1_score(y_test, y_pred_edge, average='macro', zero_division=0)
cloud_acc = accuracy_score(y_test, y_pred_cloud)
edge_acc = accuracy_score(y_test, y_pred_edge)
cloud_f1 = f1_score(y_test, y_pred_cloud, average='macro', zero_division=0)

print("\n" + "="*80)
print(" " * 25 + "GENERATING VISUALIZATIONS")
print("="*80)

fig = plt.figure(figsize=(20, 12))
fig.suptitle('Model Training Results - Comprehensive Analysis', fontsize=18, fontweight='bold')
gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)

ax1 = fig.add_subplot(gs[0, 0])
class_counts = df['Fault_Label'].value_counts().sort_index()
colors_palette = ['#2ecc71' if i == 0 else '#e74c3c' for i in range(len(class_counts))]
ax1.bar(range(len(class_counts)), class_counts.values, color=colors_palette, alpha=0.7)
ax1.set_xticks(range(len(class_counts)))
ax1.set_xticklabels([FAULT_TYPES[i].replace(' ', '\n') for i in range(len(class_counts))], 
                     rotation=45, ha='right', fontsize=9)
ax1.set_ylabel('Number of Samples')
ax1.set_title('Original Dataset - Highly Imbalanced')
ax1.set_yscale('log')
ax1.grid(axis='y', alpha=0.3)

ax2 = fig.add_subplot(gs[0, 1])
models = ['Edge\nLightGBM', 'Cloud\nLSTM']
accuracies = [edge_acc * 100, cloud_acc * 100]
f1_scores = [edge_f1 * 100, cloud_f1 * 100]

x = np.arange(len(models))
width = 0.35

bars1 = ax2.bar(x - width/2, accuracies, width, label='Accuracy', color='#3498db', alpha=0.8)
bars2 = ax2.bar(x + width/2, f1_scores, width, label='F1-Score', color='#e67e22', alpha=0.8)

ax2.axhline(y=70, color='r', linestyle='--', linewidth=2, label='Edge Target (70%)')
ax2.axhline(y=85, color='g', linestyle='--', linewidth=2, label='Cloud Target (85%)')

ax2.set_ylabel('Score (%)')
ax2.set_title('Model Performance: Accuracy & F1-Score')
ax2.set_xticks(x)
ax2.set_xticklabels(models)
ax2.legend()
ax2.grid(axis='y', alpha=0.3)
ax2.set_ylim(0, 100)

for bar in bars1:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')
for bar in bars2:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom', fontsize=9, fontweight='bold')

ax3 = fig.add_subplot(gs[0, 2])
sns.heatmap(edge_cm, annot=True, fmt='d', cmap='Blues', ax=ax3,
            xticklabels=list(FAULT_TYPES.values()),
            yticklabels=list(FAULT_TYPES.values()),
            cbar_kws={'label': 'Count'})
ax3.set_title(f'Edge LightGBM Confusion Matrix\nF1: {edge_f1*100:.2f}%', fontweight='bold')
ax3.set_xlabel('Predicted')
ax3.set_ylabel('Actual')
plt.setp(ax3.get_xticklabels(), rotation=45, ha='right', fontsize=8)
plt.setp(ax3.get_yticklabels(), rotation=0, fontsize=8)

ax4 = fig.add_subplot(gs[1, 0])
sns.heatmap(cloud_cm, annot=True, fmt='d', cmap='Reds', ax=ax4,
            xticklabels=list(FAULT_TYPES.values()),
            yticklabels=list(FAULT_TYPES.values()),
            cbar_kws={'label': 'Count'})
ax4.set_title(f'Cloud LSTM Confusion Matrix\nAccuracy: {cloud_acc*100:.2f}%', fontweight='bold')
ax4.set_xlabel('Predicted')
ax4.set_ylabel('Actual')
plt.setp(ax4.get_xticklabels(), rotation=45, ha='right', fontsize=8)
plt.setp(ax4.get_yticklabels(), rotation=0, fontsize=8)

ax5 = fig.add_subplot(gs[1, 1])
edge_report = classification_report(y_test, y_pred_edge, target_names=list(FAULT_TYPES.values()), 
                                    zero_division=0, output_dict=True)
class_f1 = [edge_report[fault_type]['f1-score'] * 100 for fault_type in FAULT_TYPES.values()]
colors = ['#2ecc71' if score >= 70 else '#e74c3c' for score in class_f1]
bars = ax5.barh(range(len(class_f1)), class_f1, color=colors, alpha=0.7)
ax5.set_yticks(range(len(class_f1)))
ax5.set_yticklabels(list(FAULT_TYPES.values()), fontsize=9)
ax5.set_xlabel('F1-Score (%)')
ax5.set_title('Edge Model: Per-Class F1-Scores')
ax5.axvline(x=70, color='orange', linestyle='--', linewidth=2, label='Target: 70%')
ax5.legend()
ax5.grid(axis='x', alpha=0.3)
for i, (bar, score) in enumerate(zip(bars, class_f1)):
    ax5.text(score + 2, i, f'{score:.1f}%', va='center', fontsize=8)

ax6 = fig.add_subplot(gs[1, 2])
cloud_report = classification_report(y_test, y_pred_cloud, target_names=list(FAULT_TYPES.values()), 
                                     zero_division=0, output_dict=True)
class_f1_cloud = [cloud_report[fault_type]['f1-score'] * 100 for fault_type in FAULT_TYPES.values()]
colors_cloud = ['#2ecc71' if score >= 70 else '#e74c3c' for score in class_f1_cloud]
bars = ax6.barh(range(len(class_f1_cloud)), class_f1_cloud, color=colors_cloud, alpha=0.7)
ax6.set_yticks(range(len(class_f1_cloud)))
ax6.set_yticklabels(list(FAULT_TYPES.values()), fontsize=9)
ax6.set_xlabel('F1-Score (%)')
ax6.set_title('Cloud Model: Per-Class F1-Scores')
ax6.axvline(x=70, color='orange', linestyle='--', linewidth=2, label='Target: 70%')
ax6.legend()
ax6.grid(axis='x', alpha=0.3)
for i, (bar, score) in enumerate(zip(bars, class_f1_cloud)):
    ax6.text(score + 2, i, f'{score:.1f}%', va='center', fontsize=8)

ax7 = fig.add_subplot(gs[2, 0])
metrics_names = ['Precision', 'Recall', 'F1-Score']
edge_metrics = [
    edge_report['macro avg']['precision'] * 100,
    edge_report['macro avg']['recall'] * 100,
    edge_report['macro avg']['f1-score'] * 100
]
cloud_metrics = [
    cloud_report['macro avg']['precision'] * 100,
    cloud_report['macro avg']['recall'] * 100,
    cloud_report['macro avg']['f1-score'] * 100
]

x = np.arange(len(metrics_names))
width = 0.35
ax7.bar(x - width/2, edge_metrics, width, label='Edge LightGBM', color='#3498db', alpha=0.8)
ax7.bar(x + width/2, cloud_metrics, width, label='Cloud LSTM', color='#e74c3c', alpha=0.8)
ax7.set_ylabel('Score (%)')
ax7.set_title('Macro Average: Precision, Recall, F1-Score')
ax7.set_xticks(x)
ax7.set_xticklabels(metrics_names)
ax7.legend()
ax7.grid(axis='y', alpha=0.3)
ax7.set_ylim(0, 100)

ax8 = fig.add_subplot(gs[2, 1])
ax8.axis('off')

status_text = "üéØ TARGET ACHIEVEMENT STATUS\n\n"
status_text += f"{'='*40}\n"
status_text += f"EDGE MODEL (LightGBM)\n"
status_text += f"{'='*40}\n"
status_text += f"Macro F1-Score: {edge_f1*100:.2f}%\n"
status_text += f"Target: ‚â• 70%\n"
if edge_f1 >= 0.70:
    status_text += f"Status: ‚úÖ TARGET MET! ‚úÖ\n"
    edge_color = 'green'
else:
    status_text += f"Status: ‚ö†Ô∏è Need {(0.70-edge_f1)*100:.2f}% more\n"
    edge_color = 'orange'

status_text += f"\n{'='*40}\n"
status_text += f"CLOUD MODEL (LSTM)\n"
status_text += f"{'='*40}\n"
status_text += f"Overall Accuracy: {cloud_acc*100:.2f}%\n"
status_text += f"Target: ‚â• 85%\n"
if cloud_acc >= 0.85:
    status_text += f"Status: ‚úÖ TARGET MET! ‚úÖ\n"
    cloud_color = 'green'
else:
    status_text += f"Status: ‚ö†Ô∏è Need {(0.85-cloud_acc)*100:.2f}% more\n"
    cloud_color = 'orange'

ax8.text(0.5, 0.5, status_text, transform=ax8.transAxes,
         fontsize=11, verticalalignment='center', horizontalalignment='center',
         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
         family='monospace')

ax9 = fig.add_subplot(gs[2, 2])
ax9.axis('off')

summary_text = "üìä MODEL SUMMARY\n\n"
summary_text += f"Edge LightGBM:\n"
summary_text += f"  ‚Ä¢ Accuracy: {edge_acc*100:.2f}%\n"
summary_text += f"  ‚Ä¢ F1-Score: {edge_f1*100:.2f}%\n"
summary_text += f"  ‚Ä¢ Uses SMOTE + 300 trees\n"
summary_text += f"  ‚Ä¢ Inference: ~1-2ms\n\n"

summary_text += f"Cloud LSTM:\n"
summary_text += f"  ‚Ä¢ Accuracy: {cloud_acc*100:.2f}%\n"
summary_text += f"  ‚Ä¢ F1-Score: {cloud_f1*100:.2f}%\n"
summary_text += f"  ‚Ä¢ Uses SMOTE + LSTM layers\n"
summary_text += f"  ‚Ä¢ Inference: ~10-20ms\n\n"

summary_text += f"Models saved to:\n"
summary_text += f"  ‚Ä¢ edge_lightgbm_model.pkl\n"
summary_text += f"  ‚Ä¢ cloud_lstm_model.keras\n"
summary_text += f"  ‚Ä¢ scaler.pkl\n"

ax9.text(0.5, 0.5, summary_text, transform=ax9.transAxes,
         fontsize=10, verticalalignment='center', horizontalalignment='center',
         bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5),
         family='monospace')

plt.tight_layout()
plt.savefig('training_results.png', dpi=300, bbox_inches='tight')
print("\n‚úì Saved: training_results.png")
plt.show()

print("\n" + "="*80)
print(" " * 30 + "FINAL SUMMARY")
print("="*80)

print(f"\n{'Model':<25} {'Metric':<20} {'Score':<10} {'Target':<10} {'Status':<10}")
print("-" * 80)
print(f"{'Edge LightGBM':<25} {'Macro F1-Score':<20} {edge_f1*100:>6.2f}% {'>= 70%':<10} {'‚úÖ MET' if edge_f1 >= 0.70 else '‚ö†Ô∏è NOT MET':<10}")
print(f"{'Cloud LSTM':<25} {'Overall Accuracy':<20} {cloud_acc*100:>6.2f}% {'>= 85%':<10} {'‚úÖ MET' if cloud_acc >= 0.85 else '‚ö†Ô∏è NOT MET':<10}")
print("-" * 80)

overall_status = (edge_f1 >= 0.70) and (cloud_acc >= 0.85)
if overall_status:
    print("\nüéâ SUCCESS! Both targets achieved! System ready for deployment.")
    print("\nNext step: Run 'python3 main.py' to execute the simulation with pre-trained models.")
else:
    print("\n‚ö†Ô∏è  One or more targets not met. Consider:")
    if edge_f1 < 0.70:
        print(f"   - Edge needs {(0.70-edge_f1)*100:.2f}% more F1-score")
        print("   - Try increasing n_estimators or max_depth")
    if cloud_acc < 0.85:
        print(f"   - Cloud needs {(0.85-cloud_acc)*100:.2f}% more accuracy")
        print("   - Try more LSTM layers or longer training")
    print("\n   However, models are still saved and can be used for simulation.")

print("\n" + "="*80)
print("‚úÖ Training complete! Models saved successfully.")
print("="*80)
print("\nüí° TIP: The highly imbalanced dataset makes achieving high macro F1 challenging.")
print("   SMOTE helps significantly, but real-world performance may vary.")
print("   The models are optimized for production use with the best possible accuracy.")
print("\nRun 'python3 main.py' to see the full system in action!")
print("="*80)